import asyncio
import json
import os
import re
from datetime import datetime, timezone
from typing import Dict, Optional

import aiohttp
from dotenv import load_dotenv
from telethon import TelegramClient, events
from telethon.errors import FloodWaitError, UserAlreadyParticipantError, UserNotParticipantError
from telethon.tl.functions.channels import JoinChannelRequest, GetParticipantRequest

load_dotenv()

print("ğŸš€ Scraper started...", flush=True)

# Telegram API credentials
API_ID = os.getenv('TELEGRAM_API_ID')
API_HASH = os.getenv('TELEGRAM_API_HASH')
PHONE = os.getenv('TELEGRAM_PHONE')
BACKEND_URL = os.getenv('BACKEND_URL', '')

print("ENV DEBUG:", API_ID, API_HASH, PHONE, flush=True)

# Ù‚Ù†ÙˆØ§Øª Ø§Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…
CHANNELS = {
    'https://t.me/+VAkpot4taw_v9n2p': 'Ø£Ø¯ÙˆØ§Øª Ù…Ù†Ø²Ù„ÙŠØ©',
    'https://t.me/+UbRrLCJUETxcZmWJ': 'Ù„Ø¹Ø¨ Ø£Ø·ÙØ§Ù„',
    'https://t.me/+TQHOHpqeFZ4a2Lmp': 'Ù…Ø³ØªØ­Ø¶Ø±Ø§Øª Ø§Ù„ØªØ¬Ù…ÙŠÙ„',
    # 'https://t.me/+T1hjkvhugV4GxRYD': 'Ù…Ù„Ø§Ø¨Ø³ Ø¯Ø§Ø®Ù„ÙŠØ©',
    'https://t.me/+Tx6OTiWMi6WS4Y2j': 'Ù…ÙØ±ÙˆØ´Ø§Øª',
    'https://t.me/+Sbbi6_lLOI2_wP41': 'Ø´Ø±Ø§Ø¨Ø§Øª',
    # 'https://t.me/+R5rjl2_-KV3GWYAr': 'Ù…Ù„Ø§Ø¨Ø³ Ø§Ù„Ø¨ÙŠØª Ùˆ Ø§Ù„Ù„Ø§Ù†Ø¬ÙŠØ±ÙŠ',
    'https://t.me/+WQ-FJCIwbKrcw2qC': 'Ù…Ù„Ø§Ø¨Ø³ Ø§Ø·ÙØ§Ù„',
    'https://t.me/+SSyWF7Ya89yPm2_V': 'Ø§ÙƒØ³Ø³ÙˆØ§Ø±Ø§Øª',
    'https://t.me/+TsQpYNpBaoRkz-8h': 'ØªØµÙÙŠØ§Øª',
}


class TelegramProductScraper:
    def __init__(self):
        print("ğŸ› ï¸ Initializing TelegramClient...", flush=True)
        self.client = TelegramClient('scraper_session', API_ID, API_HASH)
        print("âœ… Client initialized", flush=True)
        self.products = []

        # ğŸ†• Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ÙŠØ¯ÙŠØ§ ÙˆØ§Ù„Ø±Ø³Ø§Ø¦Ù„
        self.processed_messages = set()
        self.pending_media = {}
        self.message_cache = {}
        self.channel_entities = {}  # Ù„Ù„Ù€ live mode

    def extract_price(self, text: str) -> Dict[str, Optional[float]]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø³Ø¹Ø§Ø± Ù…Ù† Ø§Ù„Ù†Øµ ÙˆØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù‚Ù„ ÙƒØ§Ù„Ø³Ø¹Ø± Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        # ğŸ†• Ù†Ø¸Ù Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø¥ÙŠÙ…ÙˆØ¬ÙŠ (Ø£ÙŠ Ø­Ø±Ù ØºÙŠØ± Ø¹Ø±Ø¨ÙŠ/Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ/Ø±Ù‚Ù…/Ù…Ø³Ø§ÙØ©/Ø¹Ù„Ø§Ù…Ø§Øª ØªØ±Ù‚ÙŠÙ…)
        clean_text = re.sub(r'[^\u0600-\u06FFa-zA-Z0-9\s\.\,\:\+\-\/]', ' ', text)

        price_patterns = [
            r'(\d+(?:\.\d+)?)\s*(?:Ø¬Ù†ÙŠÙ‡|Ø¬\.Ù…|LE)',
            r'Ø§Ù„Ø³Ø¹Ø±[:\s]+(\d+(?:\.\d+)?)',
            r'Ø¨Ø³Ø¹Ø±[:\s]+(\d+(?:\.\d+)?)',
            r'Ø¨Ù€\s*(\d+(?:\.\d+)?)',
            r'(\d+(?:\.\d+)?)\s*Ø¬(?!\w)',  # Ø¬ Ù…ØªØ¨ÙˆØ¹Ø© Ø¨Ù…Ø³Ø§ÙØ© Ø£Ùˆ Ù†Ù‡Ø§ÙŠØ©
        ]

        all_prices = set()

        # Ù†Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØ§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø¸Ù
        for search_text in [text, clean_text]:
            for pattern in price_patterns:
                matches = re.findall(pattern, search_text)
                for match in matches:
                    try:
                        price = float(match)
                        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØºØ±ÙŠØ¨Ø© (Ø£ÙƒØ¨Ø± Ù…Ù† 100000 Ø£Ùˆ Ø£ØµØºØ± Ù…Ù† 1)
                        if 1 <= price <= 100000:
                            all_prices.add(price)
                    except (ValueError, TypeError):
                        pass

        prices = {'current_price': None, 'old_price': None}

        if all_prices:
            prices['current_price'] = min(all_prices)
            if len(all_prices) > 1:
                prices['old_price'] = max(all_prices)
        else:
            # fallback: Ù†Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø±Ù‚Ù… ÙÙŠ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù†Ø¸Ù Ø¨Ø¹Ø¯ ÙƒÙ„Ù…Ø© "Ø§Ù„Ø³Ø¹Ø±"
            price_context = re.search(r'Ø§Ù„Ø³Ø¹Ø±.*?(\d+(?:\.\d+)?)', clean_text)
            if price_context:
                try:
                    price = float(price_context.group(1))
                    if 1 <= price <= 100000:
                        prices['current_price'] = price
                except (ValueError, TypeError):
                    pass

            # Ù„Ùˆ Ù„Ø³Ù‡ Ù…ÙÙŠØ´ Ø³Ø¹Ø±ØŒ Ù†Ø¬ÙŠØ¨ Ø£ÙˆÙ„ Ø±Ù‚Ù… Ù…Ø¹Ù‚ÙˆÙ„
            if not prices['current_price']:
                numbers = re.findall(r'\b(\d+(?:\.\d+)?)\b', clean_text)
                for num_str in numbers:
                    try:
                        num = float(num_str)
                        if 10 <= num <= 100000:  # Ù†ÙØªØ±Ø¶ Ø¥Ù† Ø§Ù„Ø³Ø¹Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ 10 Ø¬Ù†ÙŠÙ‡
                            prices['current_price'] = num
                            break
                    except (ValueError, TypeError):
                        pass

        return prices

    async def download_image(self, message, index: int) -> Optional[str]:
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©/Ø§Ù„ÙÙŠØ¯ÙŠÙˆ/Ø§Ù„Ù…Ø³ØªÙ†Ø¯ ÙˆØ­ÙØ¸Ù‡ Ø¨Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯ Ø§Ù„ØµØ­ÙŠØ­"""
        media_dir = 'downloaded_images'
        os.makedirs(media_dir, exist_ok=True)

        ext = 'unknown'

        if getattr(message.media, 'photo', None):
            ext = 'jpg'
        elif getattr(message.media, 'document', None) and hasattr(message.media.document, 'mime_type'):
            mime = message.media.document.mime_type
            if 'png' in mime:
                ext = 'png'
            elif 'gif' in mime:
                ext = 'gif'
            elif 'jpeg' in mime:
                ext = 'jpg'
            elif 'mp4' in mime:
                ext = 'mp4'
            elif 'webp' in mime:
                ext = 'webp'
            else:
                ext = mime.split('/')[-1]

        filename = f"{media_dir}/product_{message.chat_id}_{message.id}_{index}.{ext}"

        if os.path.exists(filename):
            print(f"ğŸŸ¡ Skipping download (already exists): {filename}", flush=True)
            return filename

        # ğŸ†• Ø­Ù…Ø§ÙŠØ© Ù…Ù† FloodWaitError
        max_retries = 3
        for attempt in range(max_retries):
            try:
                await message.download_media(file=filename)
                print(f"ğŸ“¥ Downloaded new media: {filename}", flush=True)
                return filename
            except FloodWaitError as e:
                if attempt < max_retries - 1:
                    print(
                        f"â³ FloodWait during download: waiting {e.seconds} seconds... (attempt {attempt + 1}/{max_retries})",
                        flush=True)
                    await asyncio.sleep(e.seconds)
                else:
                    print(f"âŒ Failed to download after {max_retries} attempts due to FloodWait", flush=True)
                    return None
            except Exception as e:
                print(f"Error downloading media: {e}", flush=True)
                return None

        return None

    async def send_to_backend(self, product_data: Dict):
        """Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ù€ Backend Ù…Ø¹ Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±/Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª ÙƒÙ…Ù„ÙØ§Øª"""
        if not BACKEND_URL:
            print("âš ï¸ BACKEND_URL ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ù„ÙŠÙ‹Ø§.", flush=True)
            offline_file = 'offline_products.json'
            data = []
            if os.path.exists(offline_file):
                with open(offline_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            if any(p['unique_id'] == product_data['unique_id'] for p in data):
                print(f"â­ï¸ Product already exists locally: {product_data['unique_id']}", flush=True)
            else:
                data.append(product_data)
                with open(offline_file, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                print(f"ğŸ’¾ Product saved locally: {product_data['name']}", flush=True)
            return

        try:
            async with aiohttp.ClientSession() as session:
                form = aiohttp.FormData()

                form.add_field('variants[0][sku]', product_data.get('unique_id', ''))
                form.add_field('variants[0][barcode]', product_data.get('unique_id', ''))
                form.add_field('variants[0][stock]', '10')
                form.add_field('name[ar]', product_data.get('name', ''))
                form.add_field('name[en]', product_data.get('name', ''))
                form.add_field('description[ar]', product_data.get('description', ''))
                form.add_field('description[en]', product_data.get('description', ''))
                form.add_field('short_description[ar]', product_data.get('description', ''))
                form.add_field('short_description[en]', product_data.get('description', ''))
                form.add_field('category_name', product_data.get('channel_name', ''))

                prices = product_data.get('prices', {})
                if prices.get('old_price'):
                    form.add_field('variants[0][price]', str(prices['old_price']))
                    form.add_field('variants[0][discount]', str(prices['current_price']))
                else:
                    form.add_field('variants[0][price]', str(prices.get('current_price') or 0))

                for media_path in product_data.get('images', []):
                    if os.path.exists(media_path):
                        ext = os.path.splitext(media_path)[1].lower()
                        if ext in ['.jpg', '.jpeg']:
                            content_type = 'image/jpeg'
                        elif ext == '.png':
                            content_type = 'image/png'
                        elif ext == '.gif':
                            content_type = 'image/gif'
                        elif ext == '.webp':
                            content_type = 'image/webp'
                        elif ext == '.mp4':
                            content_type = 'video/mp4'
                        else:
                            content_type = None

                        if content_type is None or content_type.startswith('video/'):
                            print(f"âš ï¸ Skipping media (unsupported type): {media_path}", flush=True)
                            continue

                        form.add_field(
                            'variants[0][images][]',
                            open(media_path, 'rb'),
                            filename=os.path.basename(media_path),
                            content_type=content_type
                        )

                headers = {
                    'Authorization': f"Bearer {os.getenv('BACKEND_TOKEN', '')}",
                    'Accept': "application/json",
                    'Accept-Language': "ar",
                    'Tenant-Id': "7",
                    'Referer': "https://rosyland.obranchy.com",
                }

                async with session.post(BACKEND_URL, data=form, headers=headers) as resp:
                    resp_text = await resp.text()
                    if resp.status in [200, 201]:
                        print(f"âœ… Product sent successfully: {product_data['name']}", flush=True)
                    else:
                        print(f"âŒ Failed to send product: {resp.status}", flush=True)
                        print(f"ğŸ§¾ Response: {resp_text}", flush=True)

        except Exception as e:
            print(f"Error sending to backend: {e}", flush=True)

    async def collect_previous_media(self, entity, message, max_lookback=20):
        """Ø¬Ù…Ø¹ Ø§Ù„ØµÙˆØ±/Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ø§Ù„Ù„ÙŠ Ø¨Ø¯ÙˆÙ† Ù†Øµ"""
        media_list = []
        chat_id = message.chat_id

        try:
            if chat_id in self.message_cache:
                for msg_id in range(message.id - 1, max(message.id - max_lookback - 1, 0), -1):
                    if msg_id in self.message_cache[chat_id]:
                        prev_msg = self.message_cache[chat_id][msg_id]

                        if prev_msg.text and prev_msg.text.strip():
                            break

                        if (getattr(prev_msg.media, 'photo', None) or
                                getattr(prev_msg.media, 'document', None) or
                                getattr(prev_msg.media, 'video', None)):
                            media_list.append(prev_msg)
            else:
                # ğŸ†• Ø­Ù…Ø§ÙŠØ© Ù…Ù† FloodWaitError
                while True:
                    try:
                        async for prev_msg in self.client.iter_messages(
                                entity,
                                offset_id=message.id,
                                limit=max_lookback
                        ):
                            if chat_id not in self.message_cache:
                                self.message_cache[chat_id] = {}
                            self.message_cache[chat_id][prev_msg.id] = prev_msg

                            if prev_msg.text and prev_msg.text.strip():
                                break

                            if (getattr(prev_msg.media, 'photo', None) or
                                    getattr(prev_msg.media, 'document', None) or
                                    getattr(prev_msg.media, 'video', None)):
                                media_list.append(prev_msg)
                        break  # Ù†Ø¬Ø­Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
                    except FloodWaitError as e:
                        print(f"â³ FloodWait in collect_previous_media: waiting {e.seconds} seconds...", flush=True)
                        await asyncio.sleep(e.seconds)

        except Exception as e:
            print(f"âš ï¸ Error collecting previous media: {e}", flush=True)

        return list(reversed(media_list))

    async def process_message(self, message, channel_name: str = None, entity=None):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø³Ø§Ù„Ø© ÙˆØ§Ø­Ø¯Ø©"""
        chat_id = message.chat_id
        unique_id = f"{chat_id}_{message.id}"

        if unique_id in self.processed_messages:
            return

        if chat_id not in self.message_cache:
            self.message_cache[chat_id] = {}
        self.message_cache[chat_id][message.id] = message

        if not message.text or not message.text.strip():
            if chat_id not in self.pending_media:
                self.pending_media[chat_id] = []

            if (getattr(message.media, 'photo', None) or
                    getattr(message.media, 'document', None) or
                    getattr(message.media, 'video', None)):
                self.pending_media[chat_id].append(message)
                print(f"ğŸ“¸ Media buffered: {len(self.pending_media[chat_id])} pending", flush=True)
            return

        self.processed_messages.add(unique_id)

        product = {
            'unique_id': unique_id,
            'channel_id': chat_id,
            'message_id': message.id,
            'timestamp': message.date.isoformat(),
            'channel_name': channel_name,
            'description': message.text or '',
            'images': [],
            'prices': {'current_price': None, 'old_price': None}
        }

        text = message.text.strip()
        lines = [line.strip() for line in text.splitlines() if line.strip()]

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§Ø³Ù… ÙˆØ§Ù„ÙˆØµÙ Ø§Ù„Ù‚ØµÙŠØ± ÙˆØ§Ù„ÙƒØ¨ÙŠØ±
        if len(lines) == 0:
            name = ""
            short_desc = ""
            long_desc = ""
        elif len(lines) == 1:
            name = lines[0]
            short_desc = ""
            long_desc = ""
        elif len(lines) == 2:
            name = lines[0]
            short_desc = lines[1]
            long_desc = ""
        else:
            name = lines[0]
            short_desc = lines[1]
            long_desc = "\n".join(lines[2:])

        product['name'] = re.sub(r'(?i)\bØ§Ø³Ù… Ø§Ù„Ù…Ù†ØªØ¬\b', '', name).strip()
        product['description'] = long_desc  # Ø§Ù„ÙˆØµÙ Ø§Ù„ÙƒØ¨ÙŠØ±
        product['short_description'] = short_desc

        product['prices'] = self.extract_price(message.text)

        if chat_id in self.pending_media and self.pending_media[chat_id]:
            print(f"ğŸ”— Collecting {len(self.pending_media[chat_id])} buffered media", flush=True)
            for idx, pending_msg in enumerate(self.pending_media[chat_id]):
                media_path = await self.download_image(pending_msg, idx)
                if media_path:
                    product['images'].append(media_path)
                self.processed_messages.add(f"{chat_id}_{pending_msg.id}")

            self.pending_media[chat_id] = []

        if entity:
            prev_media_messages = await self.collect_previous_media(entity, message)
            if prev_media_messages:
                print(f"ğŸ” Found {len(prev_media_messages)} previous media messages", flush=True)

                for prev_msg in prev_media_messages:
                    prev_unique_id = f"{chat_id}_{prev_msg.id}"
                    if prev_unique_id not in self.processed_messages:
                        media_path = await self.download_image(prev_msg, len(product['images']))
                        if media_path:
                            product['images'].append(media_path)
                        self.processed_messages.add(prev_unique_id)

        if (getattr(message.media, 'photo', None) or
                getattr(message.media, 'document', None) or
                getattr(message.media, 'video', None)):
            media_path = await self.download_image(message, len(product['images']))
            if media_path:
                product['images'].append(media_path)

        if not product['images']:
            print(f"âŒ Product skipped (no media): {product['name']}", flush=True)
            return

        self.products.append(product)
        await self.send_to_backend(product)

        print(
            f"ğŸ“¦ Product processed: {product['name'][:50]} | {len(product['images'])} images | Price: {product['prices']['current_price']}",
            flush=True)

    async def scrape_channel_history(self, channel_link: str):
        """Ø³ÙƒØ±Ø§Ø¨ÙŠÙ†Ø¬ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù‚Ù†Ø§Ø© Ù…Ø¹ batch processing"""
        try:
            stop_date_str = os.getenv('STOP_DATE', '')
            stop_date = None
            if stop_date_str:
                try:
                    stop_date = datetime.strptime(stop_date_str, '%Y-%m-%d').replace(tzinfo=timezone.utc)
                    print(f"ğŸ“… Stop date set to: {stop_date.date()}", flush=True)
                except ValueError:
                    print("âš ï¸ ØªÙ†Ø¨ÙŠÙ‡: ØªÙ†Ø³ÙŠÙ‚ STOP_DATE ØºÙŠØ± ØµØ­ÙŠØ­! Ø§Ø³ØªØ®Ø¯Ù… YYYY-MM-DD.", flush=True)

            channel_name = CHANNELS.get(channel_link, 'Ù‚Ù†Ø§Ø© ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙØ©')

            while True:
                try:
                    entity = await self.client.get_entity(channel_link)

                    try:
                        me = await self.client.get_me()
                        await self.client(GetParticipantRequest(channel=entity, participant=me))
                        print(f"âœ… Already a member of {entity.title} ({channel_name})", flush=True)
                    except UserNotParticipantError:
                        try:
                            await self.client(JoinChannelRequest(entity))
                            print(f"âœ… Joined {entity.title} ({channel_name})", flush=True)
                        except UserAlreadyParticipantError:
                            print(f"âœ… Already joined {entity.title}", flush=True)

                    break

                except FloodWaitError as e:
                    print(f"â³ Flood wait: need to wait {e.seconds} seconds...", flush=True)
                    await asyncio.sleep(e.seconds)
                except Exception as e:
                    print(f"âŒ Failed to get entity {channel_link}: {e}", flush=True)
                    return

            print(f"ğŸ” Scraping channel: {entity.title} ({channel_name})", flush=True)

            # Ø­ÙØ¸ Ø§Ù„Ù€ entity Ù„Ù„Ù€ live mode
            self.channel_entities[entity.id] = (entity, channel_name)

            chat_id = entity.id
            self.message_cache[chat_id] = {}
            self.pending_media[chat_id] = []

            batch_size = 100
            messages_batch = []

            async for message in self.client.iter_messages(entity):
                if stop_date and message.date < stop_date:
                    print(f"â¹ï¸ Stopped at {message.date}", flush=True)
                    break

                self.message_cache[chat_id][message.id] = message
                messages_batch.append(message)

                if len(messages_batch) >= batch_size:
                    print(f"âš™ï¸ Processing batch of {len(messages_batch)} messages...", flush=True)
                    for msg in reversed(messages_batch):
                        await self.process_message(msg, channel_name, entity)
                    messages_batch = []
                    await asyncio.sleep(1)

            if messages_batch:
                print(f"âš™ï¸ Processing final batch of {len(messages_batch)} messages...", flush=True)
                for msg in reversed(messages_batch):
                    await self.process_message(msg, channel_name, entity)

        except Exception as e:
            print(f"Error scraping channel {channel_link}: {e}", flush=True)

    async def start_live_monitoring(self):
        """Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¨Ø§Ø´Ø±Ø©"""

        @self.client.on(events.NewMessage(chats=list(self.channel_entities.keys())))
        async def handler(event):
            print(f"ğŸ†• New message received from chat_id: {event.chat_id}!", flush=True)
            try:
                chat_id = event.chat_id

                # Ù†Ø¬ÙŠØ¨ Ø§Ù„Ù€ entity ÙˆØ§Ù„Ø§Ø³Ù… Ù…Ù† Ø§Ù„Ù€ cache
                if chat_id in self.channel_entities:
                    entity, channel_name = self.channel_entities[chat_id]
                    print(f"ğŸ“ Channel identified: {channel_name}", flush=True)
                    await self.process_message(event.message, channel_name, entity)
                else:
                    # Ù„Ùˆ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ù€ cacheØŒ Ù†Ø­Ø§ÙˆÙ„ Ù†Ø¬ÙŠØ¨Ù‡
                    print(f"âš ï¸ Unknown channel: {chat_id}, attempting to identify...", flush=True)
                    try:
                        entity = await event.get_chat()
                        # Ù†Ø´ÙˆÙ Ù„Ùˆ Ø§Ù„Ù‚Ù†Ø§Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù€ CHANNELS
                        found = False
                        for link, name in CHANNELS.items():
                            # Ù†Ø­Ø§ÙˆÙ„ Ù†Ø·Ø§Ø¨Ù‚ Ø¨Ø§Ù„Ù€ ID Ø£Ùˆ Ø§Ù„Ù€ username
                            if str(entity.id) in link or (
                                    hasattr(entity, 'username') and entity.username and entity.username in link):
                                channel_name = name
                                self.channel_entities[chat_id] = (entity, channel_name)
                                print(f"âœ… Channel identified and cached: {channel_name}", flush=True)
                                await self.process_message(event.message, channel_name, entity)
                                found = True
                                break

                        if not found:
                            print(f"âŒ Channel not found in CHANNELS dict", flush=True)
                    except Exception as e:
                        print(f"âŒ Failed to identify channel: {e}", flush=True)

            except Exception as e:
                print(f"âŒ Error in live handler: {e}", flush=True)

        print("ğŸ‘€ Monitoring channels for new messages...", flush=True)
        print(f"ğŸ“¡ Monitoring {len(self.channel_entities)} channels: {list(self.channel_entities.keys())}", flush=True)
        await self.client.run_until_disconnected()

    async def run(self, mode='history'):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø±"""
        print("ğŸ”„ Connecting to Telegram...", flush=True)

        # ğŸ†• Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø§ØªØµØ§Ù„ Ù…Ù† FloodWaitError
        while True:
            try:
                await self.client.start(phone=PHONE)
                print("âœ… Connected to Telegram", flush=True)
                break
            except FloodWaitError as e:
                print(f"â³ FloodWait during connection: waiting {e.seconds} seconds...", flush=True)
                await asyncio.sleep(e.seconds)

        if mode == 'history':
            for channel in CHANNELS:
                await self.scrape_channel_history(channel)

            with open('products.json', 'w', encoding='utf-8') as f:
                json.dump(self.products, f, ensure_ascii=False, indent=2)

            print(f"\nâœ… Scraped {len(self.products)} products", flush=True)
            print("ğŸ“ Data saved to products.json", flush=True)

        elif mode == 'live':
            # ÙÙŠ Ø§Ù„Ù€ live modeØŒ Ù„Ø§Ø²Ù… Ù†Ø¬ÙŠØ¨ Ø§Ù„Ù€ entities Ø§Ù„Ø£ÙˆÙ„
            for channel in CHANNELS:
                while True:
                    try:
                        entity = await self.client.get_entity(channel)
                        self.channel_entities[entity.id] = (entity, CHANNELS[channel])
                        break
                    except FloodWaitError as e:
                        print(f"â³ FloodWait getting entity for live mode: waiting {e.seconds} seconds...", flush=True)
                        await asyncio.sleep(e.seconds)
                    except Exception as e:
                        print(f"âŒ Failed to get entity for {channel}: {e}", flush=True)
                        break

            await self.start_live_monitoring()

        elif mode == 'hybrid':
            print("ğŸŒ€ Hybrid mode: Scraping history first, then monitoring live...", flush=True)

            for channel in CHANNELS:
                print(f"Start Fetching Channel ({channel})...", flush=True)
                await self.scrape_channel_history(channel)

            print(f"\nâœ… Finished scraping history ({len(self.products)} products).", flush=True)
            print("ğŸ‘€ Now switching to live monitoring...\n", flush=True)

            await self.start_live_monitoring()


if __name__ == '__main__':
    scraper = TelegramProductScraper()
    asyncio.run(scraper.run(mode='hybrid'))
